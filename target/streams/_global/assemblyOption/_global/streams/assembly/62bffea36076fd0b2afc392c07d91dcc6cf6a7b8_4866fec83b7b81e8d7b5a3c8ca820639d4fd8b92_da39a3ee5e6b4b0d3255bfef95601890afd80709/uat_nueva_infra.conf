
uat_nueva_infra {

  include "ofertador-defaults.conf"


  influx.database {
    name = "eventsDB"
    host = "172.30.8.52"
    port = 32128
    #host = "172.18.171.223"
    #port = 32128

  }

  kafka {
    bootstrap.servers = "172.30.8.61:9092,172.30.8.62:9092,172.30.8.66:9092"
    # bootstrap.servers = "172.30.36.29:31092"
    topic-events = "tracking-events-topic"
  }


  teradata {
    env_id = "test"
    include "teradata-desa.conf"
  }

  k8s.context = "we-uat"
  k8s.namescapce = "ofertador"
  k8s.pod.prefix ="ofertador-api"
  k8s.deployment.name="ofertador-api"

  config.files = ["api", "channel", "etl", "models", "sources", "spark", "tracker", "banner", "newbizrules"]

  hdfs-namenode = "hdfs://namenode:9000/"

  #test-ruts {
  #  minRut = 9999000
  #  maxRut = 10000000
  #}

// fechasSourcesOverride {
//   rentabilidad_source = "2020/02/03"
//    txp_source = "2020/04/13"
//    cctaf01_source = "2020/04/13"
//    sbif_source = "2020/04/13"
//    creditos_consumo_source = "2020/03/13"
//    compras_source = "2020/03/13"
//    recencias_source = "2020/04/13"
//    segmentacion_sku_source = "2020/04/13"
//    ventas_source = "2020/03/13"
//    sku_source = "2020/04/13"
//    demografico_source = "2020/03/11"
//    cuentas_source = "2020/03/11"
//    creditos_source = "2020/02/02"
//    clientes_activos_source = "2020/03/11"
//    repactaciones_source = "2020/03/11"
//    activity_and_fuga_source = "2020/02/02"
//    aumento_cupo_source = "2020/03/11"
//    venta_tarj_source = "2020/04/13"
//    segmentacion_source = "2020/03/12"
//    preventivo_source = "2020/04/17"
//  }


  cat.read {
    # Conexion a DB CAT
    user = "admdbcat"
    password = "admdbcat"
    url = "jdbc:postgresql://172.18.171.231:5432/dbcat_og"
    driver = "org.postgresql.Driver"
    batchsize = 200  # number of rows per insert (Spark)
    customBatchsize = 2  # numero de filas por insert
    isolationLevel = "READ_UNCOMMITTED"  # "NONE", "READ_COMMITTED", "READ_UNCOMMITTED" (default), "REPEATABLE_READ", "SERIALIZABLE"
    maximumPoolSize = 2  # connection pool de postgres
  }

  cat.write {
    # Conexion a DB CAT
    user = "admdbcat"
    password = "admdbcat"
    url = "jdbc:postgresql://172.18.171.231:5432/dbcat_og"
    driver = "org.postgresql.Driver"
    batchsize = 200  # number of rows per insert (Spark)
    customBatchsize = 2  # numero de filas por insert
    isolationLevel = "READ_UNCOMMITTED"  # "NONE", "READ_COMMITTED", "READ_UNCOMMITTED" (default), "REPEATABLE_READ", "SERIALIZABLE"
    maximumPoolSize = 2  # connection pool de postgres
  }


  prediction.model.root = "models:"
  prediction.model.root = ${?PREDICTION_MODEL_ROOT}

  //Es el de WE_PROD, el único al que llega spark de uat de la nueva infra. TODO, cambiar cuando esté k8s arreglado.
  config-server = "localhost"
  config-server-port = 8887

  error-reporting {
    # Directorios donde se van a generar los dos archivos de error (por jobName)
    rootPath = "/tmp"
  }

  // num of spark partitions
  spark {
    num_partitions  = 10
    app {
      name = "spark-etl-cctaf02"
    }
  }

  elastic {
    host = "localhost"
    port = 9200
    db = "cencosud_index"
  }

  path_prefix = "."

  redis {
    lock = true
    readTimeout = 240
    redis-sentinel = {

      sentinels: ["172.30.8.64:26379","172.30.8.65:26379"]//, "172.30.8.63:26379"]
      masterName = "mymaster"
    }

    redis-instances = [
      {
        host="172.30.8.64"
        port="6379"
      }
      {
        host="172.30.8.65"
        port="6379"
      }
//      {
//        host="172.30.8.65"
//        port="6379"
//      }
    ]
    pool-config {
      maxTotal = 128
      maxIdle = 128
      minIdle = 16
      testOnBorrow = true
      testOnReturn = true
      testWhileIdle = true
      minEvictableIdleTimeSeconds = 60
      timeBetweenEvictionRunsSeconds = 30
      numTestsPerEvictionRun = 3
      blockWhenExhausted = true
      maxWaitSeconds = 256
    }
  }

  green {

    readTimeout = 240
    redis {

      redis-sentinel = {

        sentinels: ["172.30.8.64:26380","172.30.8.65:26380"]//, "172.30.8.63:26379"]
        masterName = "mymaster"
      }

      redis-instances = [
        {
          host="172.30.8.64"
          port="6380"
        }
        {
          host="172.30.8.65"
          port="6380"
        }
//        {
//          host="172.30.8.65"
//          port="6379"
//        }
      ]
      pool-config {
        maxTotal = 128
        maxIdle = 128
        minIdle = 16
        testOnBorrow = true
        testOnReturn = true
        testWhileIdle = true
        minEvictableIdleTimeSeconds = 60
        timeBetweenEvictionRunsSeconds = 30
        numTestsPerEvictionRun = 3
        blockWhenExhausted = true
        maxWaitSeconds = 256
      }
    }
  }

  blue {
    redis {

      readTimeout = 240
      redis-sentinel = {

        sentinels: ["172.30.8.64:26379","172.30.8.65:26379"]//, "172.30.8.63:26379"]
        masterName = "mymaster"
      }
      redis-instances = [
        {
          host="172.30.8.64"
          port="6379"
        }
        {
          host="172.30.8.65"
          port="6379"
        }
//        {
//          host="172.30.8.65"
//          port="6379"
//        }
      ]
      pool-config {
        maxTotal = 128
        maxIdle = 128
        minIdle = 16
        testOnBorrow = true
        testOnReturn = true
        testWhileIdle = true
        minEvictableIdleTimeSeconds = 60
        timeBetweenEvictionRunsSeconds = 30
        numTestsPerEvictionRun = 3
        blockWhenExhausted = true
        maxWaitSeconds = 256
      }
    }
  }



}
